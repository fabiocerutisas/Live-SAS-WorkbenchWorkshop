{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Modelling\n",
    "This notebook walks through the process of developing ML models both in sk-learn and in sasviya.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GradientBoostingSK\n",
    "from sasviya.ml.tree import ForestClassifier, GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../Data/cleaned_data/train_valid_test.csv'\n",
    "numerical_features = ['CreditLineAge','DebtIncRatio','FICOScore','Inquiries6Mnths',\n",
    "                      'LogAnnualInc','RevBalance','RevUtilization','Installment','InterestRate']\n",
    "categorical_features = ['CreditPolicy','Delinquencies2Yrs','PublicRecord','Purpose']\n",
    "features = numerical_features+categorical_features\n",
    "target = 'Default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(data_path)\n",
    "\n",
    "train = all_data[all_data['_PartInd_']==1].reset_index(drop=True)\n",
    "valid = all_data[all_data['_PartInd_']==2].reset_index(drop=True)\n",
    "test = all_data[all_data['_PartInd_']==3].reset_index(drop=True)\n",
    "\n",
    "train_defaults = train[target].sum()\n",
    "valid_defaults = valid[target].sum()\n",
    "test_defaults = test[target].sum()\n",
    "print('Train Size:', train.shape[0], f'--- {target} Frequency:', f'{round(100*train_defaults/train.shape[0],2)}%')\n",
    "print('Valid Size:', valid.shape[0], f'--- {target} Frequency:', f'{round(100*valid_defaults/valid.shape[0],2)}%')\n",
    "print('Test Size:', test.shape[0], f'--- {target} Frequency:', f'{round(100*test_defaults/test.shape[0],2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SK-Learn Pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    (\"standard_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"ordinal_encoder\", OrdinalEncoder()),\n",
    "])\n",
    "\n",
    "sk_pipeline = Pipeline([\n",
    "    (\"data_prep\", ColumnTransformer([(\"num_pipeline\", num_pipeline, numerical_features),(\"cat_pipeline\", cat_pipeline, categorical_features)])),\n",
    "    ('Forest_Model', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "sk_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the pipeline\n",
    "sk_pipeline.fit(train[features], train[target])\n",
    "#Save predictions \n",
    "train_pred_sk = sk_pipeline.predict(train[features])\n",
    "valid_pred_sk = sk_pipeline.predict(valid[features])\n",
    "test_pred_sk = sk_pipeline.predict(test[features])\n",
    "#Compute Fit Metrics\n",
    "train_f1_sk = f1_score(train[target], train_pred_sk)\n",
    "valid_f1_sk = f1_score(valid[target], valid_pred_sk)\n",
    "test_f1_sk = f1_score(test[target], test_pred_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viya_pipeline = Pipeline([\n",
    "    (\"data_prep\", ColumnTransformer([(\"num_pipeline\", num_pipeline, numerical_features),(\"cat_pipeline\", cat_pipeline, categorical_features)])),\n",
    "    ('Forest_Model', ForestClassifier(random_state=42))])\n",
    "\n",
    "viya_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the pipeline\n",
    "viya_pipeline.fit(train[features], train[target])\n",
    "#Save predictions \n",
    "train_pred_viya = viya_pipeline.predict(train[features])\n",
    "valid_pred_viya = viya_pipeline.predict(valid[features])\n",
    "test_pred_viya = viya_pipeline.predict(test[features])\n",
    "#Compute Fit Metrics\n",
    "train_f1_viya = f1_score(train[target], train_pred_viya)\n",
    "valid_f1_viya = f1_score(valid[target], valid_pred_viya)\n",
    "test_f1_viya = f1_score(test[target], test_pred_viya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = pd.DataFrame(\n",
    "    {'Forest-SK': [train_f1_sk, valid_f1_sk, test_f1_sk], \n",
    "     'Forest-Viya': [train_f1_viya, valid_f1_viya, test_f1_viya]},\n",
    "    ['Train F1', 'Valid F1', 'Test F1'])\n",
    "100*model_comparison.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(20,5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_pred_sk, normalize='pred'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[0])\n",
    "axs[0].set_title('Test Confusion Matrix - SK')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_pred_viya, normalize='pred'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[1])\n",
    "axs[1].set_title('Test Confusion Matrix - Viya')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "\n",
    "Develop a Gradient Boosting Model, test different hyperparameters and compare its performance against the above trained Logistic and Forest Models.\n",
    "\n",
    "![GB Classifier Overview](../../img/GB_Details_Python.png)\n",
    "\n",
    "For further guidance: https://go.documentation.sas.com/doc/en/workbenchcdc/v_001/explore/n1kiea90s0276wn1xr0ig0hvkix6.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, fit it and evaluate it\n",
    "your_gb_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to compute the Train F1, Valid F1 and Test F1\n",
    "model_comparison = pd.DataFrame(\n",
    "    {'Forest-SK': [train_f1_sk, valid_f1_sk, test_f1_sk], \n",
    "     'Forest-Viya': [train_f1_viya, valid_f1_viya, test_f1_viya],\n",
    "     'GB': [_________]},\n",
    "    ['Train F1', 'Valid F1', 'Test F1'])\n",
    "100*model_comparison.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce confusion matrices to compare performance against the Logistic Regression and the Gradient Boosting\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20,5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_pred_sk, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[0])\n",
    "axs[0].set_title('Test Confusion Matrix - Logistic')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_pred_viya, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[1])\n",
    "axs[1].set_title('Test Confusion Matrix - Forest')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], _______, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[2])\n",
    "axs[2].set_title('Test Confusion Matrix - GB')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's fit only the models for registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep = Pipeline([(\"data_prep\", ColumnTransformer([(\"num_pipeline\", num_pipeline, numerical_features),(\"cat_pipeline\", cat_pipeline, categorical_features)]))])\n",
    "data_prep.fit(train[features])\n",
    "x_train_prep = pd.DataFrame(data_prep.transform(train[features]), columns=features)\n",
    "x_valid_prep = pd.DataFrame(data_prep.transform(valid[features]), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_viya_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_viya_model.fit(x_train_prep, train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = GradientBoostingSK(random_state=42)\n",
    "sk_model.fit(x_train_prep, train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models\n",
    "We create an artifacts folder and save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'artifacts'\n",
    "if not os.path.isdir(mypath):\n",
    "   os.makedirs(mypath)\n",
    "\n",
    "with open('artifacts/sklearn_forest_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump(sk_pipeline, file)\n",
    "\n",
    "with open('artifacts/viya_forest_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump(viya_pipeline, file)\n",
    "\n",
    "with open('artifacts/data_prep.pkl', 'wb') as file:\n",
    "    pickle.dump(data_prep, file)\n",
    "\n",
    "with open('artifacts/sk_model.pkl', 'wb') as file:\n",
    "    pickle.dump(sk_model, file)\n",
    "\n",
    "gb_viya_model.save('artifacts/gb_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
