{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sasviya.ml.linear_model import LogisticRegression\n",
    "from sasviya.ml.tree import ForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sampler=TPESampler(seed=SEED)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting and Modelling\n",
    "In this section we start by defining key variables such as:\n",
    "- data_path = here we pass the path to the modelling_data we have previously prepared \n",
    "- columns_to_exclude = we specify which columns from the set of available ones we should exclude when fitting the models\n",
    "- target = specify the target variable\n",
    "- train_frac = portion of data for training\n",
    "- valid_frac = portion of data for validation\n",
    "- test_frac = portion of data for testing\n",
    "\n",
    "Next, we split the data into train, validation and test by stratifying on the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/cleaned_data/train_valid_test.csv'\n",
    "features = ['CreditPolicy', 'PublicRecord',  'Purpose', 'InterestRate', 'Installment','Delinquencies2Yrs', \n",
    "            'BIN_CreditLineAge', 'BIN_DebtIncRatio', 'BIN_FICOScore','BIN_Inquiries6Mnths', 'BIN_LogAnnualInc', \n",
    "            'BIN_RevBalance','BIN_RevUtilization']\n",
    "target = 'Default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_transform_credit_data = pd.read_csv(data_path)\n",
    "\n",
    "train = woe_transform_credit_data[woe_transform_credit_data['_PartInd_']==1].reset_index(drop=True)\n",
    "valid = woe_transform_credit_data[woe_transform_credit_data['_PartInd_']==2].reset_index(drop=True)\n",
    "test = woe_transform_credit_data[woe_transform_credit_data['_PartInd_']==3].reset_index(drop=True)\n",
    "\n",
    "train_defaults = train[target].sum()\n",
    "valid_defaults = valid[target].sum()\n",
    "test_defaults = test[target].sum()\n",
    "print('Train Size:', train.shape[0], f'--- {target} Frequency:', f'{round(100*train_defaults/train.shape[0],2)}%')\n",
    "print('Valid Size:', valid.shape[0], f'--- {target} Frequency:', f'{round(100*valid_defaults/valid.shape[0],2)}%')\n",
    "print('Test Size:', test.shape[0], f'--- {target} Frequency:', f'{round(100*test_defaults/test.shape[0],2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are defining some key functions to automate the hyperparameter search and the entire model fitting.\n",
    "\n",
    "For instance:\n",
    "- objective_logistic() => is a function that defines the search space for the logistic regression hyperparameters. In a given trial, we will then fit the model and return the performance metric we are trying to maximize/minimize.\n",
    "- optimize_model() => creates the optimization study, produces validation metrics and returns the train F1 score, the validation F1 score, the best hyperparameters and the best probability cutoff to include.\n",
    "- test_fit() => performance a fit on test data of the developed model\n",
    "- optimize_p_threshold() => iterates over a set of possible probability cutoffs and identifies the ones that maximizes performance on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_logistic(trial, train, valid):\n",
    "    selection = trial.suggest_categorical('selection', [\"backward\", \"forward\", \"lasso\", \"stepwise\", None])\n",
    "    model = LogisticRegression(selection=selection)\n",
    "    model.fit(train[features], train[target])\n",
    "    preds = model.predict(valid[features])\n",
    "    return f1_score(valid[target].to_numpy(), preds)\n",
    "\n",
    "def objective_forest(trial, train, valid):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 300, step=50)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 20)\n",
    "    max_features = trial.suggest_categorical('max_features', [len(features), int(len(features)**0.5), int(np.log2(len(features)))])\n",
    "    n_bins = trial.suggest_int('n_bins', 10, 100, step=10)\n",
    "    model = ForestClassifier(n_estimators=n_estimators, max_depth=max_depth,max_features=max_features, \n",
    "                             n_bins=n_bins, random_state=SEED, oob_score=False)\n",
    "    model.fit(train[features], train[target])\n",
    "    preds = model.predict(valid[features])\n",
    "    return f1_score(valid[target].to_numpy(), preds)\n",
    "\n",
    "def optimize_model(model, n_trials, train, valid):\n",
    "    study = optuna.create_study(direction='maximize', sampler = sampler)\n",
    "    if model == 'Logistic':\n",
    "        study.optimize(lambda trial: objective_logistic(trial, train, valid), n_trials=n_trials)\n",
    "        optimized_model = LogisticRegression(**study.best_params)\n",
    "    if model == 'Forest':\n",
    "        study.optimize(lambda trial: objective_forest(trial, train, valid), n_trials=n_trials)\n",
    "        optimized_model = ForestClassifier(**study.best_params)\n",
    "\n",
    "    optimized_model.fit(train[features], train[target])\n",
    "    prob_preds = optimized_model.predict_proba(valid[features]).iloc[:,1].to_numpy()\n",
    "    valid_f1, best_prob_thresh = optimize_p_threshold(prob_preds, valid[target])\n",
    "    train_preds = optimized_model.predict_proba(train[features]).iloc[:,1].to_numpy()\n",
    "    train_preds[train_preds>=best_prob_thresh] = 1\n",
    "    train_preds[train_preds<best_prob_thresh] = 0\n",
    "    train_f1 = f1_score(train[target].to_numpy(), train_preds)\n",
    "    print(f'Finished {model} optimization')\n",
    "    return train_f1, valid_f1, study.best_params, best_prob_thresh\n",
    "\n",
    "def test_fit(model, hyperparams, train_valid, best_prob_thresh):\n",
    "    train_valid = pd.concat([train, valid])\n",
    "    if model == 'Logistic':\n",
    "        optimized_model = LogisticRegression(**hyperparams)\n",
    "    if model == 'Forest':\n",
    "        optimized_model = ForestClassifier(**hyperparams)\n",
    "    \n",
    "    optimized_model.fit(train_valid[features], train_valid[target])\n",
    "    prob_preds = optimized_model.predict_proba(test[features]).iloc[:,1].to_numpy()\n",
    "    prob_preds[prob_preds>=best_prob_thresh] = 1\n",
    "    prob_preds[prob_preds<best_prob_thresh] = 0\n",
    "    test_f1 = f1_score(test[target].to_numpy(), prob_preds)\n",
    "    return optimized_model, test_f1, prob_preds\n",
    "\n",
    "def optimize_p_threshold(prob_preds, true_y):\n",
    "    best_prob_thresh = 0\n",
    "    best_f1_achieved = 0\n",
    "    for i in np.arange(0.01, 1, 0.01):\n",
    "        transf_preds = prob_preds.copy()\n",
    "        transf_preds[transf_preds>=i] = 1\n",
    "        transf_preds[transf_preds<i] = 0\n",
    "        curr_f1 = f1_score(true_y, transf_preds)\n",
    "        if curr_f1 > best_f1_achieved:\n",
    "            best_prob_thresh=i\n",
    "            best_f1_achieved = curr_f1\n",
    "    return best_f1_achieved, best_prob_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f1_logistic, valid_f1_logistic, logistic_hp, prob_thresh_logistic = optimize_model('Logistic', 20, train, valid)\n",
    "train_f1_forest, valid_f1_forest, forest_hp, prob_thresh_forest = optimize_model('Forest', 20, train, valid)\n",
    "\n",
    "train_valid = pd.concat([train, valid])\n",
    "\n",
    "optimized_logistic, test_f1_logistic, test_logistic_preds = test_fit('Logistic', logistic_hp, train_valid, prob_thresh_logistic)\n",
    "optimized_forest, test_f1_forest, test_forest_preds = test_fit('Forest', forest_hp, train_valid, prob_thresh_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = pd.DataFrame(\n",
    "    {'Logistic': [train_f1_logistic, valid_f1_logistic, test_f1_logistic, prob_thresh_logistic], \n",
    "    'Forest': [train_f1_forest, valid_f1_forest, test_f1_forest, prob_thresh_forest]},\n",
    "    ['Train F1', 'Valid F1', 'Test F1', 'Prob Threshold'])\n",
    "100*model_comparison.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(20,5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_logistic_preds, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[0])\n",
    "axs[0].set_title('Test Confusion Matrix - Logistic')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_forest_preds, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[1])\n",
    "axs[1].set_title('Test Confusion Matrix - Forest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "\n",
    "Develop a Gradient Boosting Model, optimize it and compare its performance against the above trained Logistic and Forest Models.\n",
    "\n",
    "![GB Classifier Overview](../img/GB_Details_Python.png)\n",
    "\n",
    "For further guidance: https://go.documentation.sas.com/doc/en/workbenchcdc/v_001/explore/n1kiea90s0276wn1xr0ig0hvkix6.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sasviya.ml.tree import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, fit it and evaluate it\n",
    "# Note: Optuna can suggest the following: suggest_int, suggest_float, suggest_categorical\n",
    "def objective_gb(trial, train, valid):\n",
    "    #Your Hyperparameters definition\n",
    "\n",
    "    model = GradientBoostingClassifier(YOUR HYPERPARAMETERS)\n",
    "    model.fit(train[features], train[target])\n",
    "    preds = model.predict(valid[features])\n",
    "    return f1_score(valid[target].to_numpy(), preds)\n",
    "\n",
    "def optimize_gb_model(model, n_trials, train, valid):\n",
    "    study = optuna.create_study(direction='maximize', sampler = sampler)\n",
    "    study.optimize(lambda trial: objective_gb(trial, train, valid), n_trials=n_trials)\n",
    "    optimized_model = GradientBoostingClassifier(**study.best_params)\n",
    "    optimized_model.fit(train[features], train[target])\n",
    "    prob_preds = optimized_model.predict_proba(valid[features]).iloc[:,1].to_numpy()\n",
    "    valid_f1, best_prob_thresh = optimize_p_threshold(prob_preds, valid[target])\n",
    "    train_preds = optimized_model.predict_proba(train[features]).iloc[:,1].to_numpy()\n",
    "    train_preds[train_preds>=best_prob_thresh] = 1\n",
    "    train_preds[train_preds<best_prob_thresh] = 0\n",
    "    train_f1 = f1_score(train[target].to_numpy(), train_preds)\n",
    "    print(f'Finished {model} optimization')\n",
    "    return train_f1, valid_f1, study.best_params, best_prob_thresh\n",
    "\n",
    "def test_fit_gb(model, hyperparams, train_valid, best_prob_thresh):\n",
    "    train_valid = pd.concat([train, valid])\n",
    "    optimized_model = GradientBoostingClassifier(**hyperparams)  \n",
    "    optimized_model.fit(train_valid[features], train_valid[target])\n",
    "    prob_preds = optimized_model.predict_proba(test[features]).iloc[:,1].to_numpy()\n",
    "    prob_preds[prob_preds>=best_prob_thresh] = 1\n",
    "    prob_preds[prob_preds<best_prob_thresh] = 0\n",
    "    test_f1 = f1_score(test[target].to_numpy(), prob_preds)\n",
    "    return optimized_model, test_f1, prob_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, fit it and evaluate it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to compute the Train F1, Valid F1, Test F1 and Prob Threshold\n",
    "model_comparison = pd.DataFrame(\n",
    "    {'Logistic': [train_f1_logistic, valid_f1_logistic, test_f1_logistic, prob_thresh_logistic], \n",
    "    'Forest': [train_f1_forest, valid_f1_forest, test_f1_forest, prob_thresh_forest],\n",
    "    'GB': [_____]},\n",
    "    ['Train F1', 'Valid F1', 'Test F1', 'Prob Threshold'])\n",
    "100*model_comparison.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce confusion matrices to compare performance against the Logistic Regression and the Gradient Boosting\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(20,5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_logistic_preds, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[0])\n",
    "axs[0].set_title('Test Confusion Matrix - Logistic')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_forest_preds, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[1])\n",
    "axs[1].set_title('Test Confusion Matrix - Forest')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], ____, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[2])\n",
    "axs[2].set_title('Test Confusion Matrix - GB')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_path = '../data/cleaned_data/synthetic_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = pd.read_csv(synthetic_data_path)\n",
    "\n",
    "train = pd.concat([train, synthetic_data])\n",
    "train_defaults = train[target].sum()\n",
    "valid_defaults = valid[target].sum()\n",
    "test_defaults = test[target].sum()\n",
    "print('Train Size:', train.shape[0], f'--- {target} Frequency:', f'{round(100*train_defaults/train.shape[0],2)}%')\n",
    "print('Valid Size:', valid.shape[0], f'--- {target} Frequency:', f'{round(100*valid_defaults/valid.shape[0],2)}%')\n",
    "print('Test Size:', test.shape[0], f'--- {target} Frequency:', f'{round(100*test_defaults/test.shape[0],2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "\n",
    "Train a GradientBoosting Classifier with the newly augmented train dataset, optimize it and evaluate its performance.\n",
    "\n",
    "- Are there any deltas in the Fit Metrics?\n",
    "- Produce a table to directly compare fit metrics with and without synthetic data\n",
    "- Produce the newly achieved confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = pd.DataFrame(\n",
    "    {'Logistic': [train_f1_logistic, valid_f1_logistic, test_f1_logistic, prob_thresh_logistic], \n",
    "    'Forest': [train_f1_forest, valid_f1_forest, test_f1_forest, prob_thresh_forest],\n",
    "    'GB': [_______],\n",
    "    'GB Synth': [_____]},\n",
    "    ['Train F1', 'Valid F1', 'Test F1', 'Prob Threshold'])\n",
    "100*model_comparison.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(20,5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_logistic_preds, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[0])\n",
    "axs[0].set_title('Test Confusion Matrix - Logistic')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], test_forest_preds, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[1])\n",
    "axs[1].set_title('Test Confusion Matrix - Forest')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], ______, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[2])\n",
    "axs[2].set_title('Test Confusion Matrix - GB')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test[target], ______, normalize='true'))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=axs[3])\n",
    "axs[3].set_title('Test Confusion Matrix - GB+Synth')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models\n",
    "We create an artifacts folder and save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'artifacts'\n",
    "if not os.path.isdir(mypath):\n",
    "   os.makedirs(mypath)\n",
    "\n",
    "optimized_logistic.save('artifacts/logistics_model.pkl')\n",
    "optimized_forest.save('artifacts/forest_model.pkl')\n",
    "# Your GB Model\n",
    "# Your GB+Synth Model\n",
    "model_comparison.to_csv('artifacts/model_comparison.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
